############################# Server Basics #############################
<% if not @kafka_brokers.nil? %>
    <% @ip_addresses = Chef::IPFinder.find(node, :private_ipv4) %>
    <% @kafka_brokers = @kafka_brokers.split(',') %>
    <% @ip_addresses.each do |ip| %>
        <% @node.override[:kafka][:broker_id] = @kafka_brokers.include?(ip) ? @kafka_brokers.index(ip) : 0 %>
        <% break if not (@myid == 0) %>
    <% end %>
<% end %>

# The id of the broker. This must be set to a unique integer for each broker.
broker.id=<%=node[:kafka][:broker_id]%>

############################# Socket Server Settings #############################

# The port the socket server listens on
port=<%=node[:kafka][:port]%>

# The number of processor threads the socket server uses for receiving vag  and answering requests. 
# Defaults to the number of cores on the machine
<% if (node[:kafka][:threads].nil? || node[:kafka][:threads].empty?) %>
#num.threads
<% else %>
num.threads=<%=node[:kafka][:threads]%>
<% end %>

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer=1048576

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer=1048576

# The maximum size of a request that the socket server will accept (protection against OOM)
max.socket.request.bytes=104857600


############################# Log Basics #############################

# The directory under which to store kafka data
log.dirs=<%=node[:kafka][:data_dir]%>

############################# Log Flush Policy #############################

# The number of messages to accept before forcing a flush of data to disk
log.flush.interval=<%=node[:kafka][:log_flush_interval]%>

# The maximum amount of time a message can sit in a log before we force a flush
log.default.flush.interval.ms=<%=node[:kafka][:log_flush_time_interval]%>

# Per-topic overrides for log.default.flush.interval.ms
#topic.flush.intervals.ms=topic1:1000, topic2:3000

# The interval (in ms) at which logs are checked to see if they need to be flushed to disk.
log.default.flush.scheduler.interval.ms=<%=node[:kafka][:log_flush_scheduler_time_interval]%>

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion
log.retention.hours=<%=node[:kafka][:log_retention_hours]%>

# A size-based retention policy for logs. Segments are pruned from the log as long as the remaining
# segments don't drop below log.retention.size.
#log.retention.size=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.file.size=536870912

# The interval at which log segments are checked to see if they can be deleted according 
# to the retention policies
log.cleanup.interval.mins=1

############################# Zookeeper #############################
<% if @zk_servers.nil? %>
    <% @zk_servers = '127.0.0.1' %>
<% end %>

# Zk connection string (see zk docs for details).
zookeeper.connect=<%= @zk_servers.split(',').join(':2181,') %>:2181
